{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b99bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced RAG Libraries Loaded.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 1: INSTALL & SETUP\n",
    "# ==========================================\n",
    "# !pip install rank_bm25 sentence-transformers faiss-cpu\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from groq import Groq\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import string\n",
    "import json\n",
    "from rag_evaluation import RAGEvaluator\n",
    "\n",
    "# Load Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Define Model Constants\n",
    "EMBED_MODEL_ID = \"BAAI/bge-small-en-v1.5\"\n",
    "# LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
    "LLM_MODEL = \"openai/gpt-oss-120b\"\n",
    "\n",
    "# Initialize Groq Client\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "print(\"âœ… Advanced RAG Libraries Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbc5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The CrossEncoder `automodel_args` argument was renamed and is now deprecated, please use `model_kwargs` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Checking for Re-Ranker at: ./models/rerank_model...\n",
      "   â€¢ Found local model cache. Loading from disk...\n",
      "âš ï¸ Local load failed (sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder._load_model() got multiple values for keyword argument 'local_files_only'). Re-downloading...\n",
      "âœ… Re-Ranker downloaded and repaired.\n"
     ]
    }
   ],
   "source": [
    "# 2. DEFINE MODEL & PATHS\n",
    "RERANK_MODEL_ID = \"cross-encoder/ms-marco-MiniLM-L-6-v2\" \n",
    "LOCAL_RERANK_PATH = \"./models/rerank_model\"\n",
    "\n",
    "print(f\"â³ Checking for Re-Ranker at: {LOCAL_RERANK_PATH}...\")\n",
    "\n",
    "# 3. LOAD OR DOWNLOAD LOGIC\n",
    "if os.path.exists(LOCAL_RERANK_PATH):\n",
    "    print(\"   â€¢ Found local model cache. Loading from disk...\")\n",
    "    try:\n",
    "        # Load from local folder to avoid internet dependency\n",
    "        cross_encoder = CrossEncoder(LOCAL_RERANK_PATH, automodel_args={\"local_files_only\": True})\n",
    "        print(\"âœ… Re-Ranker loaded from local cache.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Local load failed ({e}). Re-downloading...\")\n",
    "        cross_encoder = CrossEncoder(RERANK_MODEL_ID)\n",
    "        cross_encoder.save(LOCAL_RERANK_PATH)\n",
    "        print(\"âœ… Re-Ranker downloaded and repaired.\")\n",
    "else:\n",
    "    print(f\"   â€¢ Local cache not found. Downloading {RERANK_MODEL_ID}...\")\n",
    "    # Download from Hub\n",
    "    cross_encoder = CrossEncoder(RERANK_MODEL_ID)\n",
    "    # Save to local folder for next time\n",
    "    print(f\"   â€¢ Saving model to {LOCAL_RERANK_PATH}...\")\n",
    "    cross_encoder.save(LOCAL_RERANK_PATH)\n",
    "    print(\"âœ… Re-Ranker downloaded and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f705166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Total:     24.00 GB\n",
      "System Available: 10.19 GB\n",
      "System Used:      57.5%\n",
      "Notebook Usage:   0.63 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    # 1. Overall System Memory\n",
    "    system_mem = psutil.virtual_memory()\n",
    "    print(f\"System Total:     {system_mem.total / (1024**3):.2f} GB\")\n",
    "    print(f\"System Available: {system_mem.available / (1024**3):.2f} GB\")\n",
    "    print(f\"System Used:      {system_mem.percent}%\")\n",
    "    \n",
    "    # 2. Specific Process Memory (This Notebook)\n",
    "    process = psutil.Process(os.getpid())\n",
    "    process_mem = process.memory_info().rss  # RSS = Resident Set Size (actual RAM used)\n",
    "    print(f\"Notebook Usage:   {process_mem / (1024**3):.2f} GB\")\n",
    "\n",
    "get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf225a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Initializing Advanced Retrieval Engine...\n",
      "   â€¢ Loading Embedding Model from local cache: ./models/embedding_model...\n",
      "   â€¢ Loaded 39141 documents from FAISS.\n",
      "   â€¢ Found cached BM25 Index at ./models/bm25_index.pkl. Loading...\n",
      "âœ… Hybrid System Ready (Dense + Sparse).\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 2: THE ADVANCED RETRIEVER (Hybrid)\n",
    "# ==========================================\n",
    "\n",
    "class AdvancedRetriever:\n",
    "    def __init__(self, model_path=\"./models\"):\n",
    "        print(\"âš™ï¸ Initializing Advanced Retrieval Engine...\")\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "        # --- 1. Load Embedding Model (Locally Cached) ---\n",
    "        embed_model_path = f\"{model_path}/embedding_model\"\n",
    "        \n",
    "        if os.path.exists(embed_model_path):\n",
    "            print(f\"   â€¢ Loading Embedding Model from local cache: {embed_model_path}...\")\n",
    "            self.encoder = SentenceTransformer(embed_model_path)\n",
    "        else:\n",
    "            print(f\"   â€¢ Downloading Embedding Model ({EMBED_MODEL_ID})...\")\n",
    "            self.encoder = SentenceTransformer(EMBED_MODEL_ID)\n",
    "            print(f\"   â€¢ Saving model to {embed_model_path} for future runs...\")\n",
    "            self.encoder.save(embed_model_path)\n",
    "        \n",
    "        # --- 2. Load FAISS Index (Dense) ---\n",
    "        self.index = faiss.read_index(f\"{model_path}/faiss_index.bin\")\n",
    "        \n",
    "        # --- 3. Load Metadata (Content) ---\n",
    "        with open(f\"{model_path}/chunk_metadata.pkl\", \"rb\") as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "            \n",
    "        print(f\"   â€¢ Loaded {len(self.chunks)} documents from FAISS.\")\n",
    "        \n",
    "        # --- 4. Load OR Build BM25 Index (Sparse) ---\n",
    "        bm25_path = f\"{model_path}/bm25_index.pkl\"\n",
    "        \n",
    "        if os.path.exists(bm25_path):\n",
    "            print(f\"   â€¢ Found cached BM25 Index at {bm25_path}. Loading...\")\n",
    "            with open(bm25_path, \"rb\") as f:\n",
    "                self.bm25 = pickle.load(f)\n",
    "        else:\n",
    "            print(\"   â€¢ No cache found. Building BM25 Index from scratch...\")\n",
    "            tokenized_corpus = [doc['text'].lower().split() for doc in self.chunks]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "            \n",
    "            print(f\"   â€¢ Saving BM25 Index to {bm25_path}...\")\n",
    "            with open(bm25_path, \"wb\") as f:\n",
    "                pickle.dump(self.bm25, f)\n",
    "                \n",
    "        print(\"âœ… Hybrid System Ready (Dense + Sparse).\")\n",
    "\n",
    "    def search_dense(self, query, k=50):\n",
    "        \"\"\"Standard Vector Search\"\"\"\n",
    "        # BGE requires instruction for queries\n",
    "        query_prompt = f\"Represent this sentence for searching relevant passages: {query}\"\n",
    "        query_vec = self.encoder.encode([query_prompt], normalize_embeddings=True, convert_to_numpy=True)\n",
    "        \n",
    "        D, I = self.index.search(query_vec, k)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(k):\n",
    "            idx = I[0][i]\n",
    "            # We don't trust vector scores blindly anymore, so we just store the rank\n",
    "            results.append(self.chunks[idx])\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def search_sparse(self, query, k=50):\n",
    "        \"\"\"Keyword Search using BM25\"\"\"\n",
    "        tokenized_query = query.lower().split()\n",
    "        # Get top-k documents\n",
    "        docs = self.bm25.get_top_n(tokenized_query, self.chunks, n=k)\n",
    "        return docs\n",
    "\n",
    "    def hybrid_search(self, query, k=50, rrf_k=60):\n",
    "        \"\"\"\n",
    "        Performs Reciprocal Rank Fusion (RRF) on Dense and Sparse results.\n",
    "        Returns the top-k fused results.\n",
    "        \"\"\"\n",
    "        # 1. Run both searches in parallel (conceptually)\n",
    "        dense_results = self.search_dense(query, k=k)\n",
    "        sparse_results = self.search_sparse(query, k=k)\n",
    "        \n",
    "        # 2. Score Map: {chunk_id: rrf_score}\n",
    "        # We use the text or title+text as a unique key since we don't have explicit IDs in metadata yet\n",
    "        # (Ideally, add a UUID to chunks in Phase 1, but using text hash works for now)\n",
    "        scores = {}\n",
    "        \n",
    "        # Helper to get unique key\n",
    "        def get_key(doc):\n",
    "            return hash(doc['text']) \n",
    "\n",
    "        # Map objects for easy retrieval later\n",
    "        doc_map = {}\n",
    "\n",
    "        # 3. Calculate RRF for Dense\n",
    "        for rank, doc in enumerate(dense_results):\n",
    "            doc_id = get_key(doc)\n",
    "            doc_map[doc_id] = doc\n",
    "            if doc_id not in scores: scores[doc_id] = 0\n",
    "            # RRF Formula: 1 / (k + rank)\n",
    "            scores[doc_id] += 1 / (rrf_k + rank + 1) # +1 because rank is 0-indexed\n",
    "\n",
    "        # 4. Calculate RRF for Sparse\n",
    "        for rank, doc in enumerate(sparse_results):\n",
    "            doc_id = get_key(doc)\n",
    "            doc_map[doc_id] = doc\n",
    "            if doc_id not in scores: scores[doc_id] = 0\n",
    "            scores[doc_id] += 1 / (rrf_k + rank + 1)\n",
    "            \n",
    "        # 5. Sort by Combined Score\n",
    "        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n",
    "        \n",
    "        # 6. Return Top K Docs\n",
    "        fused_results = [doc_map[uid] for uid in sorted_ids[:k]]\n",
    "        \n",
    "        return fused_results\n",
    "\n",
    "# Initialize (This might take 30-60s to build BM25)\n",
    "hybrid_retriever = AdvancedRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253dc962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Results for: 'What is error 404?'\n",
      "[1] Translation\n",
      "    Preview: Translation is the communication of the meaning of a source-language text by means of an equivalent target-language text. The English language draws a...\n",
      "[2] Xbox_360\n",
      "    Preview: To aid customers with defective consoles, Microsoft extended the Xbox 360's manufacturer's warranty to three years for hardware failure problems that ...\n",
      "[3] Wayback_Machine\n",
      "    Preview: Limitations\n",
      "In 2014, there was a six-month lag time between when a website was crawled and when it became available for viewing in the Wayback Machine...\n",
      "[4] Xbox_360\n",
      "    Preview: Timeline\n",
      "United States\n",
      "Technical problems\n",
      "The original model of the Xbox 360 has been subject to a number of technical problems. Since the console's r...\n",
      "[5] Compact_disc\n",
      "    Preview: Error scanning can reliably predict data losses caused by media deterioration. Support of error scanning differs between vendors and models of optical...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 3: TEST HYBRID RETRIEVAL\n",
    "# ==========================================\n",
    "\n",
    "test_query = \"What is error 404?\" \n",
    "# Or use a specific name from your dataset like \"Mathew Knowles\"\n",
    "\n",
    "results = hybrid_retriever.hybrid_search(test_query, k=5)\n",
    "\n",
    "print(f\"ðŸ”Ž Results for: '{test_query}'\")\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"[{i+1}] {res['title']}\")\n",
    "    print(f\"    Preview: {res['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a54c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CELL 4: INITIALIZE RE-RANKER\n",
    "# ==========================================\n",
    "def rerank_results(query, initial_results, top_k=5):\n",
    "    \"\"\"\n",
    "    Takes a list of 'candidate' chunks (e.g., top 50 from Hybrid Search),\n",
    "    scores them with the Cross-Encoder, and returns the top K best ones.\n",
    "    \"\"\"\n",
    "    if not initial_results:\n",
    "        return []\n",
    "    \n",
    "    # 1. Prepare Pairs: [[Query, Text1], [Query, Text2], ...]\n",
    "    # The model expects a list of pairs to evaluate.\n",
    "    pairs = [[query, doc['text']] for doc in initial_results]\n",
    "    \n",
    "    # 2. Score Pairs\n",
    "    # This is the \"expensive\" step. It reads every word.\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # 3. Attach Scores to Documents\n",
    "    ranked_results = []\n",
    "    for i, doc in enumerate(initial_results):\n",
    "        # Create a copy so we don't mess up the original list\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy['rerank_score'] = float(scores[i])\n",
    "        ranked_results.append(doc_copy)\n",
    "    \n",
    "    # 4. Sort by Score (High to Low)\n",
    "    ranked_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    # 5. Return Top K\n",
    "    return ranked_results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebed8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CELL 5: THE ADVANCED PIPELINE\n",
    "# ==========================================\n",
    "LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
    "def generate_advanced_rag_answer(query, retriever_instance, top_k_retrieval=50, top_k_final=5, llm_model=LLM_MODEL):\n",
    "    \"\"\"\n",
    "    The Advanced RAG Pipeline:\n",
    "    1. Hybrid Retrieval (Vectors + Keywords) -> Get Top 50\n",
    "    2. Re-Ranking (Cross-Encoder) -> Filter to Top 5\n",
    "    3. Generation (LLM) -> Answer\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- PHASE 1: RETRIEVAL (Wide Net) ---\n",
    "    # We fetch more docs (50) to ensure we didn't miss anything.\n",
    "    candidates = retriever_instance.hybrid_search(query, k=top_k_retrieval)\n",
    "    \n",
    "    # --- PHASE 2: RE-RANKING (Precision Filter) ---\n",
    "    # We let the Cross-Encoder pick the absolute best ones.\n",
    "    final_docs = rerank_results(query, candidates, top_k=top_k_final)\n",
    "    \n",
    "    # --- PHASE 3: CONTEXT CONSTRUCTION ---\n",
    "    context_text = \"\"\n",
    "    for i, doc in enumerate(final_docs):\n",
    "        context_text += f\"Source {i+1} [Score: {doc['rerank_score']:.4f}]:\\n{doc['text']}\\n\\n\"\n",
    "    \n",
    "    # --- PHASE 4: GENERATION ---\n",
    "    system_prompt = \"\"\"You are a precise and helpful AI assistant.\n",
    "    Use the provided Context Information to answer the user's question.\n",
    "    If the answer is not present in the context, strictly state: \"I cannot answer this based on the provided documents.\"\n",
    "    Do not hallucinate.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Context Information:\n",
    "    ---------------------\n",
    "    {context_text}\n",
    "    ---------------------\n",
    "    \n",
    "    User Question: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            model=LLM_MODEL,\n",
    "            temperature=0.1,\n",
    "            max_tokens=512\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        answer = f\"Error generating answer: {e}\"\n",
    "        \n",
    "    return answer, final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc45153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running Advanced RAG Pipeline for: 'What is the exact height of the Eiffel Tower?'\n",
      "   â€¢ Strategy: Hybrid Retrieval (Top 50) -> Cross-Encoder Re-Ranking (Top 5)\n",
      "\n",
      "ðŸ¤– Generated Answer:\n",
      "============================================================\n",
      "I cannot answer this based on the provided documents.\n",
      "============================================================\n",
      "\n",
      "ðŸ” Top 5 Re-Ranked Contexts (Sorted by Relevance Score):\n",
      "[1] Score: 2.7520 | Title: Elevator\n",
      "    Preview: Eiffel Tower\n",
      "The Eiffel Tower has Otis double-deck elevators built into the legs of the tower, serving the ground level to the first and second levels. Even though the shaft runs diagonally upwards wi...\n",
      "----------------------------------------\n",
      "[2] Score: 2.3572 | Title: Paris\n",
      "    Preview: Paris' urbanism laws have been under strict control since the early 17th century, particularly where street-front alignment, building height and building distribution is concerned. In recent developme...\n",
      "----------------------------------------\n",
      "[3] Score: -1.1881 | Title: Paris\n",
      "    Preview: Paris's urbanism laws have been under strict control since the early 17th century, particularly where street-front alignment, building height and building distribution is concerned. The 210 m (690 ft)...\n",
      "----------------------------------------\n",
      "[4] Score: -2.0093 | Title: Paris\n",
      "    Preview: In May 1968, protesting students occupied the Sorbonne and put up barricades in the Latin Quarter. Thousands of Parisian blue-collar workers joined the students, and the movement grew into a two-week ...\n",
      "----------------------------------------\n",
      "[5] Score: -3.6711 | Title: Montevideo\n",
      "    Preview: Telecommunications Tower\n",
      "Torre de las Telecomunicaciones (Telecommunications Tower) or Torre Antel (Antel Tower) is the 158 meters (518 ft), 37-floor headquarters of Uruguay's government-owned telecom...\n",
      "----------------------------------------\n",
      "\n",
      "âœ… Pipeline Test Complete.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 6: ONE-OFF INTERACTIVE TEST\n",
    "# ==========================================\n",
    "# Choose a query that requires specific facts to test the Re-Ranker's precision\n",
    "# query = \"When did Beyonce start becoming popular?\"\n",
    "# Alternative tough queries to try later:\n",
    "query = \"What is the exact height of the Eiffel Tower?\"  # Tests number precision\n",
    "# query = \"Who managed Destiny's Child?\" #(Tests entity relation)\n",
    "\n",
    "print(f\"ðŸš€ Running Advanced RAG Pipeline for: '{query}'\")\n",
    "print(f\"   â€¢ Strategy: Hybrid Retrieval (Top 50) -> Cross-Encoder Re-Ranking (Top 5)\")\n",
    "\n",
    "# 1. Execute Pipeline\n",
    "answer, final_docs = generate_advanced_rag_answer(\n",
    "    query, \n",
    "    hybrid_retriever,    # Your AdvancedRetriever instance from Cell 2\n",
    "    top_k_retrieval=50,  # Wide Net\n",
    "    top_k_final=5        # Precision Filter\n",
    ")\n",
    "\n",
    "# 2. Display Final Output\n",
    "print(\"\\nðŸ¤– Generated Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(answer)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 3. Inspect the \"Why\" (The Re-Ranker's decisions)\n",
    "print(\"\\nðŸ” Top 5 Re-Ranked Contexts (Sorted by Relevance Score):\")\n",
    "for i, doc in enumerate(final_docs):\n",
    "    print(f\"[{i+1}] Score: {doc['rerank_score']:.4f} | Title: {doc['title']}\")\n",
    "    print(f\"    Preview: {doc['text'][:200]}...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# 4. Optional: Check if we filtered out noise\n",
    "# If the scores drop off sharply (e.g., 0.98 -> 0.10), the Re-Ranker is working well.\n",
    "print(f\"\\nâœ… Pipeline Test Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8666542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced Evaluator Initialized.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 7: INITIALIZE EVALUATOR\n",
    "# ==========================================\n",
    "# Initialize with the ADVANCED components\n",
    "evaluator = RAGEvaluator(\n",
    "    retriever_instance=hybrid_retriever,        # Use the NEW Hybrid Retriever\n",
    "    generator_func=generate_advanced_rag_answer, # Use the NEW Re-Ranking Pipeline\n",
    "    groq_client=client\n",
    ")\n",
    "\n",
    "print(\"âœ… Advanced Evaluator Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b6479",
   "metadata": {},
   "source": [
    "## WITHOUT IMPOSSIBLE Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736de837",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WITHOUT IMPOSSIBLE\n",
    "with open(\"./data/raw/squad_eval_set_all.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "    \n",
    "# Filter only validation data\n",
    "# eval_dataset = raw_data\n",
    "eval_dataset = [x for x in raw_data if x['is_impossible'] == False]\n",
    "# eval_dataset = [x for x in raw_data if x['split'] == 'validation']\n",
    "print(f\"ðŸ“š Loaded {len(eval_dataset)} validation questions.\")\n",
    "\n",
    "del raw_data\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# CELL 3: RUN EXPERIMENT 1 (Baseline)\n",
    "# ==========================================\n",
    "\n",
    "# Let's run a test with 30 questions\n",
    "df, summary = evaluator.run_experiment(\n",
    "    dataset=eval_dataset,\n",
    "    experiment_name=\"not_impossible\",\n",
    "    rag_type=\"advanced_rag\",\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    # model_name=\"openai/gpt-oss-120b\",\n",
    "    sample_size=50,\n",
    "    target_rpm=15,                     # Slower for safety (Judge adds calls)\n",
    "    use_llm_judge=False,                # Enable Judge\n",
    "    # judge_model = \"llama-3.3-70b-versatile\"\n",
    "    # judge_model=\"openai/gpt-oss-20b\"\n",
    "    judge_model=\"openai/gpt-oss-120b\" # Pick your judge!\n",
    ")\n",
    "\n",
    "# View the first few rows of the saved data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ea048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30    Error generating answer: Error code: 429 - {'e...\n",
       "31    Error generating answer: Error code: 429 - {'e...\n",
       "32    Error generating answer: Error code: 429 - {'e...\n",
       "33    Error generating answer: Error code: 429 - {'e...\n",
       "34    Error generating answer: Error code: 429 - {'e...\n",
       "35    Error generating answer: Error code: 429 - {'e...\n",
       "36    Error generating answer: Error code: 429 - {'e...\n",
       "37    Error generating answer: Error code: 429 - {'e...\n",
       "38    Error generating answer: Error code: 429 - {'e...\n",
       "39    Error generating answer: Error code: 429 - {'e...\n",
       "40    Error generating answer: Error code: 429 - {'e...\n",
       "41    Error generating answer: Error code: 429 - {'e...\n",
       "42    Error generating answer: Error code: 429 - {'e...\n",
       "43    Error generating answer: Error code: 429 - {'e...\n",
       "44    Error generating answer: Error code: 429 - {'e...\n",
       "45    Error generating answer: Error code: 429 - {'e...\n",
       "46    Error generating answer: Error code: 429 - {'e...\n",
       "47    Error generating answer: Error code: 429 - {'e...\n",
       "48    Error generating answer: Error code: 429 - {'e...\n",
       "49    Error generating answer: Error code: 429 - {'e...\n",
       "Name: generated_answer, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_not_impossible = pd.read_csv(\"\")\n",
    "# df_not_impossible.to_csv(\"final_results/NI_adv_rag.csv\")\n",
    "df_not_impossible[df_not_impossible['generated_answer'].str.contains('Error')]['generated_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"final_results/NI_adv_rag.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.shape)\n",
    "df[df['generated_answer'].str.contains('Error')]['generated_answer']\n",
    "\n",
    "start = 0\n",
    "end = 10\n",
    "df_batch = df[start:end]\n",
    "print(df_batch.shape)\n",
    "df_batch.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM AS A JUDGE\n",
    "df_batch = df[start:end]\n",
    "print(df_batch.shape) ### RUN AGAIN\n",
    "\n",
    "# Run the judge\n",
    "df_judged, summary_judged = evaluator.evaluate_batch(\n",
    "    results_path_or_df=df_batch,\n",
    "    judge_model=\"openai/gpt-oss-120b\",\n",
    "    target_rpm = 15\n",
    ")\n",
    "for k, v in summary_judged.items(): print(f\"{k:<25}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_10 = pd.read_csv('')\n",
    "df0_10[df0_10['judge_reasoning'].str.contains('Error')]['judge_reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee11ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WITHOUT IMPOSSIBLE\n",
    "df0_10 = pd.read_csv('')\n",
    "df10_20 = pd.read_csv('')\n",
    "df20_30 = pd.read_csv('')\n",
    "df30_40 = pd.read_csv('')\n",
    "df40_50 = pd.read_csv('')\n",
    "df_final_NI = pd.concat([df0_10, df10_20, df20_30, df30_40, df40_50], ignore_index=True)\n",
    "df_final_NI.to_csv('final_results/NI_adv_rag_judged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e2163",
   "metadata": {},
   "source": [
    "## WITH IMPOSSIBLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WITH IMPOSSIBLE\n",
    "with open(\"./data/raw/squad_eval_set_all.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "    \n",
    "# Filter only validation data\n",
    "eval_dataset = raw_data\n",
    "# eval_dataset = [x for x in raw_data if x['is_impossible'] == False]\n",
    "# eval_dataset = [x for x in raw_data if x['split'] == 'validation']\n",
    "print(f\"ðŸ“š Loaded {len(eval_dataset)} validation questions.\")\n",
    "\n",
    "del raw_data\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# CELL 3: RUN EXPERIMENT 1 (Baseline)\n",
    "# ==========================================\n",
    "\n",
    "df1, summary = evaluator.run_experiment(\n",
    "    dataset=eval_dataset,\n",
    "    experiment_name=\"with_impossible\",\n",
    "    rag_type=\"advanced_rag\",\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    # model_name=\"openai/gpt-oss-120b\",\n",
    "    sample_size=50,\n",
    "    target_rpm=15,                     # Slower for safety (Judge adds calls)\n",
    "    use_llm_judge=False,                # Enable Judge\n",
    "    # judge_model = \"llama-3.3-70b-versatile\"\n",
    "    # judge_model=\"openai/gpt-oss-20b\"\n",
    "    judge_model=\"openai/gpt-oss-120b\" \n",
    ")\n",
    "\n",
    "# View the first few rows of the saved data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impossible = pd.read_csv(\"\")\n",
    "df_impossible.to_csv(\"final_results/Imp_adv_rag.csv\")\n",
    "df_impossible[df_impossible['generated_answer'].str.contains('Error')]['generated_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b61ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"final_results/Imp_adv_rag.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.shape)\n",
    "df[df['generated_answer'].str.contains('Error')]['generated_answer']\n",
    "\n",
    "start = 0\n",
    "end = 10\n",
    "df_batch = df[start:end]\n",
    "print(df_batch.shape)\n",
    "df_batch.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM AS A JUDGE\n",
    "df_batch = df[start:end]\n",
    "print(df_batch.shape) ### RUN AGAIN\n",
    "\n",
    "# Run the judge\n",
    "df_judged, summary_judged = evaluator.evaluate_batch(\n",
    "    results_path_or_df=df_batch,\n",
    "    judge_model=\"openai/gpt-oss-120b\",\n",
    "    target_rpm = 15\n",
    ")\n",
    "for k, v in summary_judged.items(): print(f\"{k:<25}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_10 = pd.read_csv('')\n",
    "df0_10[df0_10['judge_reasoning'].str.contains('Error')]['judge_reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c52404",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WITH IMPOSSIBLE\n",
    "df0_10 = pd.read_csv('')\n",
    "df10_20 = pd.read_csv('')\n",
    "df20_30 = pd.read_csv('')\n",
    "df30_40 = pd.read_csv('')\n",
    "df40_50 = pd.read_csv('')\n",
    "df_final_Imp = pd.concat([df0_10, df10_20, df20_30, df30_40, df40_50], ignore_index=True)\n",
    "df_final_Imp.to_csv('final_results/Imp_adv_rag_judged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02e419",
   "metadata": {},
   "source": [
    "## COMBINING BOTH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d332678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_NI = pd.read_csv('final_results/NI_adv_rag_judged.csv')\n",
    "df_final_Imp = pd.read_csv('final_results/Imp_adv_rag_judged.csv')\n",
    "print(df_final_NI.shape)\n",
    "print(df_final_Imp.shape)\n",
    "df_final = pd.concat([df_final_NI, df_final_Imp], ignore_index=True)\n",
    "print(df_final.shape)\n",
    "df_final.to_csv('final_results/adv_rag_judged.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
