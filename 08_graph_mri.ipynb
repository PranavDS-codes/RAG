{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6412855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MRI Scanner Ready.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 1: SETUP\n",
    "# ==========================================\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "from rapidfuzz import process, fuzz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Any\n",
    "\n",
    "# --- CONFIG ---\n",
    "GRAPH_PATH = \"./models/knowledge_graph.pkl\"\n",
    "FUZZY_THRESHOLD = 90  # Similarity score (0-100) to flag as potential duplicate\n",
    "\n",
    "print(\"‚úÖ MRI Scanner Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af89804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Graph from ./models/knowledge_graph.pkl...\n",
      "   üìä Graph Loaded. Total Nodes: 311,236\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 2: MRI SCANNER LOGIC\n",
    "# ==========================================\n",
    "\n",
    "class GraphMRI:\n",
    "    def __init__(self, graph_path):\n",
    "        print(f\"üìÇ Loading Graph from {graph_path}...\")\n",
    "        with open(graph_path, \"rb\") as f:\n",
    "            self.G = pickle.load(f)\n",
    "        self.nodes = list(self.G.nodes())\n",
    "        print(f\"   üìä Graph Loaded. Total Nodes: {len(self.nodes):,}\")\n",
    "\n",
    "    def _normalize_key(self, text):\n",
    "        \"\"\"\n",
    "        Aggressive normalizer: lowercases, removes all non-alphanumeric chars.\n",
    "        Examples:\n",
    "        'Jay-Z' -> 'jayz'\n",
    "        'JAY Z' -> 'jayz'\n",
    "        'Beyonc√©' -> 'beyonce'\n",
    "        \"\"\"\n",
    "        # Convert non-string nodes (like integers) to string first\n",
    "        text = str(text)\n",
    "        \n",
    "        # NFKC normalizes compatibility characters (e.g., fancy hyphens, accents)\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        \n",
    "        # Keep only alphanumeric and lowercase\n",
    "        return \"\".join([c.lower() for c in text if c.isalnum()])\n",
    "\n",
    "    def scan_normalization_collisions(self):\n",
    "        \"\"\"\n",
    "        Finds nodes that are effectively the same name but formatted differently.\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç SCAN 1: Normalization Collisions\")\n",
    "        print(\"   (Detects: 'Jay-Z' vs 'Jay Z' vs 'TIDAL' vs 'Tidal')\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Map normalized_key -> [list of actual nodes]\n",
    "        normalization_map = defaultdict(list)\n",
    "        for node in self.nodes:\n",
    "            key = self._normalize_key(node)\n",
    "            if key: \n",
    "                normalization_map[key].append(node)\n",
    "        \n",
    "        collision_count = 0\n",
    "        clusters = []\n",
    "\n",
    "        for key, candidates in normalization_map.items():\n",
    "            if len(candidates) > 1:\n",
    "                # We found a collision!\n",
    "                collision_count += 1\n",
    "                \n",
    "                # Sort by degree (connectivity) to see which is the \"Main\" node\n",
    "                candidates_with_degree = [(n, self.G.degree(n)) for n in candidates]\n",
    "                # Sort descending by degree\n",
    "                candidates_with_degree.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                clusters.append(candidates_with_degree)\n",
    "\n",
    "        # Report findings\n",
    "        if collision_count == 0:\n",
    "            print(\"   ‚úÖ No Normalization Collisions found.\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Found {collision_count} collision clusters.\")\n",
    "            print(\"   Top 10 Clusters by Impact:\")\n",
    "            \n",
    "            # Show top 10 most impactful duplicates (ignoring year numbers usually)\n",
    "            sorted_clusters = sorted(clusters, key=lambda x: x[0][1], reverse=True)\n",
    "            \n",
    "            for cluster in sorted_clusters[:10]:\n",
    "                primary_node, primary_deg = cluster[0]\n",
    "                print(f\"\\n   üö© Cluster: '{self._normalize_key(primary_node)}'\")\n",
    "                \n",
    "                for node, degree in cluster:\n",
    "                    tag = \"üèÜ MAIN\" if degree == primary_deg else \"üóëÔ∏è DUPE\"\n",
    "                    print(f\"      {tag}: '{node}' (Connections: {degree})\")\n",
    "                    \n",
    "                # Check for shared neighbors (Are they talking to the same people?)\n",
    "                if len(cluster) >= 2:\n",
    "                    n1 = set(self.G.neighbors(cluster[0][0]))\n",
    "                    n2 = set(self.G.neighbors(cluster[1][0]))\n",
    "                    shared = n1.intersection(n2)\n",
    "                    print(f\"      üîó Shared Neighbors: {len(shared)} (e.g., {list(shared)[:3]})\")\n",
    "\n",
    "    def scan_targeted_fuzzy(self, targets):\n",
    "        \"\"\"\n",
    "        Checks specific important entities for spelling variations.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîç SCAN 2: Targeted Fuzzy Scan for {targets}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert all nodes to string for rapidfuzz\n",
    "        node_strings = [str(n) for n in self.nodes]\n",
    "        \n",
    "        for target in targets:\n",
    "            # extract top 10 matches\n",
    "            matches = process.extract(target, node_strings, limit=10, scorer=fuzz.token_sort_ratio)\n",
    "            \n",
    "            suspicious = []\n",
    "            for match_name, score, index in matches:\n",
    "                # We only care about high scores that aren't exact matches\n",
    "                if score >= FUZZY_THRESHOLD and match_name != target:\n",
    "                    degree = self.G.degree(match_name)\n",
    "                    suspicious.append((match_name, score, degree))\n",
    "            \n",
    "            if suspicious:\n",
    "                print(f\"   üéØ Target: '{target}'\")\n",
    "                for cand in suspicious:\n",
    "                    print(f\"      ‚Ä¢ Potential Dupe: '{cand[0]}' (Score: {cand[1]:.1f}, Degree: {cand[2]})\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ Target '{target}': Clean (No fuzzy dupes > {FUZZY_THRESHOLD}%)\")\n",
    "        \n",
    "    def scan_redundant_edges(self):\n",
    "        \"\"\"\n",
    "        Scan 3: Finds pairs of nodes with multiple edges that are identical.\n",
    "        (Fixed to handle mixed int/str node types).\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç SCAN 3: Redundant Edge Analysis\")\n",
    "        print(\"   (Detects: Duplicate relations between same nodes)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if not self.G.is_multigraph():\n",
    "            print(\"   ‚ÑπÔ∏è Graph is not a MultiGraph. Duplicate edges are impossible.\")\n",
    "            return\n",
    "\n",
    "        multi_edge_pairs = 0\n",
    "        redundant_count = 0\n",
    "        processed_pairs = set()\n",
    "\n",
    "        # Iterate over all nodes\n",
    "        for u in self.G.nodes():\n",
    "            # Iterate over outgoing neighbors\n",
    "            for v in self.G[u]:\n",
    "                \n",
    "                # Create a unique key for this directed edge pair\n",
    "                pair_key = (u, v)\n",
    "                \n",
    "                # Skip if we've already analyzed this pair\n",
    "                if pair_key in processed_pairs:\n",
    "                    continue\n",
    "                processed_pairs.add(pair_key)\n",
    "\n",
    "                # Get all edges between u and v\n",
    "                edge_dict = self.G.get_edge_data(u, v)\n",
    "                \n",
    "                # If there is more than 1 edge key (0, 1, 2...)\n",
    "                if len(edge_dict) > 1:\n",
    "                    multi_edge_pairs += 1\n",
    "                    \n",
    "                    # Extract relations safely\n",
    "                    relations = []\n",
    "                    for d in edge_dict.values():\n",
    "                        rel = d.get('relation', 'related_to')\n",
    "                        # Ensure relation is a string before lowercasing\n",
    "                        if isinstance(rel, str):\n",
    "                            relations.append(rel.lower().strip())\n",
    "                        else:\n",
    "                            relations.append(str(rel))\n",
    "                    \n",
    "                    # Check if unique relations < total edges\n",
    "                    if len(set(relations)) < len(relations):\n",
    "                        redundant_count += 1\n",
    "\n",
    "        print(f\"   üìä Node Pairs with Multiple Edges: {multi_edge_pairs:,}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Pairs with EXACT Duplicate Relations: {redundant_count:,}\")\n",
    "\n",
    "# Initialize\n",
    "mri = GraphMRI(GRAPH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb53473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç SCAN 1: Normalization Collisions\n",
      "   (Detects: 'Jay-Z' vs 'Jay Z' vs 'TIDAL' vs 'Tidal')\n",
      "--------------------------------------------------\n",
      "   ‚ö†Ô∏è Found 15396 collision clusters.\n",
      "   Top 10 Clusters by Impact:\n",
      "\n",
      "   üö© Cluster: 'unitedstates'\n",
      "      üèÜ MAIN: 'United States' (Connections: 1907)\n",
      "      üóëÔ∏è DUPE: 'United_States' (Connections: 16)\n",
      "      üîó Shared Neighbors: 2 (e.g., ['2007', 'Myanmar'])\n",
      "\n",
      "   üö© Cluster: 'newyorkcity'\n",
      "      üèÜ MAIN: 'New York City' (Connections: 645)\n",
      "      üóëÔ∏è DUPE: 'New_York_City' (Connections: 1)\n",
      "      üîó Shared Neighbors: 1 (e.g., [\"one of the world's largest natural harbors\"])\n",
      "\n",
      "   üö© Cluster: 'ottomanempire'\n",
      "      üèÜ MAIN: 'Ottoman Empire' (Connections: 527)\n",
      "      üóëÔ∏è DUPE: 'Ottoman_Empire' (Connections: 6)\n",
      "      üóëÔ∏è DUPE: 'Ottoman empire' (Connections: 3)\n",
      "      üîó Shared Neighbors: 1 (e.g., ['1918'])\n",
      "\n",
      "   üö© Cluster: 'unitedkingdom'\n",
      "      üèÜ MAIN: 'United Kingdom' (Connections: 500)\n",
      "      üóëÔ∏è DUPE: 'United_Kingdom' (Connections: 1)\n",
      "      üîó Shared Neighbors: 0 (e.g., [])\n",
      "\n",
      "   üö© Cluster: 'queen'\n",
      "      üèÜ MAIN: 'Queen' (Connections: 486)\n",
      "      üóëÔ∏è DUPE: 'Queen +' (Connections: 3)\n",
      "      üîó Shared Neighbors: 1 (e.g., ['Robbie Williams'])\n",
      "\n",
      "   üö© Cluster: 'sovietunion'\n",
      "      üèÜ MAIN: 'Soviet Union' (Connections: 455)\n",
      "      üóëÔ∏è DUPE: 'Soviet_Union' (Connections: 8)\n",
      "      üîó Shared Neighbors: 1 (e.g., ['1991'])\n",
      "\n",
      "   üö© Cluster: 'dell'\n",
      "      üèÜ MAIN: 'Dell' (Connections: 451)\n",
      "      üóëÔ∏è DUPE: 'DELL' (Connections: 1)\n",
      "      üîó Shared Neighbors: 0 (e.g., [])\n",
      "\n",
      "   üö© Cluster: '2010'\n",
      "      üèÜ MAIN: '2010' (Connections: 450)\n",
      "      üóëÔ∏è DUPE: '20‚Äì10' (Connections: 1)\n",
      "      üîó Shared Neighbors: 0 (e.g., [])\n",
      "\n",
      "   üö© Cluster: '2008'\n",
      "      üèÜ MAIN: '2008' (Connections: 353)\n",
      "      üóëÔ∏è DUPE: '2008' (Connections: 1)\n",
      "      üîó Shared Neighbors: 0 (e.g., [])\n",
      "\n",
      "   üö© Cluster: 'oklahoma'\n",
      "      üèÜ MAIN: 'Oklahoma' (Connections: 349)\n",
      "      üóëÔ∏è DUPE: 'Oklahoma!' (Connections: 4)\n",
      "      üîó Shared Neighbors: 0 (e.g., [])\n",
      "\n",
      "üîç SCAN 2: Targeted Fuzzy Scan for ['Beyonc√©', 'Jay-Z', 'Tidal', 'Parkwood Entertainment', \"Destiny's Child\", 'Columbia Records']\n",
      "--------------------------------------------------\n",
      "   ‚úÖ Target 'Beyonc√©': Clean (No fuzzy dupes > 90%)\n",
      "   ‚úÖ Target 'Jay-Z': Clean (No fuzzy dupes > 90%)\n",
      "   ‚úÖ Target 'Tidal': Clean (No fuzzy dupes > 90%)\n",
      "   ‚úÖ Target 'Parkwood Entertainment': Clean (No fuzzy dupes > 90%)\n",
      "   ‚úÖ Target 'Destiny's Child': Clean (No fuzzy dupes > 90%)\n",
      "   üéØ Target: 'Columbia Records'\n",
      "      ‚Ä¢ Potential Dupe: 'Columbia records' (Score: 93.8, Degree: 1)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 3: RUN DIAGNOSIS\n",
    "# ==========================================\n",
    "\n",
    "# 1. Run the Normalization Scan (Finds formatting errors)\n",
    "mri.scan_normalization_collisions()\n",
    "\n",
    "# 2. Run the Fuzzy Scan (Finds typos for key entities)\n",
    "# Add entities you care about here\n",
    "key_entities = [\n",
    "    \"Beyonc√©\", \n",
    "    \"Jay-Z\", \n",
    "    \"Tidal\", \n",
    "    \"Parkwood Entertainment\", \n",
    "    \"Destiny's Child\",\n",
    "    \"Columbia Records\"\n",
    "]\n",
    "\n",
    "mri.scan_targeted_fuzzy(key_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b3a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Patient Graph from ./models/knowledge_graph.pkl...\n",
      "   üìä Initial Nodes: 311,236\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 4: THE GRAPH SURGEON (PRUNING ENGINE)\n",
    "# ==========================================\n",
    "\n",
    "class GraphSurgeon:\n",
    "    def __init__(self, graph_path):\n",
    "        self.graph_path = graph_path\n",
    "        print(f\"üìÇ Loading Patient Graph from {graph_path}...\")\n",
    "        with open(graph_path, \"rb\") as f:\n",
    "            self.G = pickle.load(f)\n",
    "        self.initial_count = self.G.number_of_nodes()\n",
    "        print(f\"   üìä Initial Nodes: {self.initial_count:,}\")\n",
    "\n",
    "    def _normalize_key(self, text):\n",
    "        \"\"\"Same normalization logic as the MRI scanner.\"\"\"\n",
    "        text = str(text)\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        return \"\".join([c.lower() for c in text if c.isalnum()])\n",
    "\n",
    "    def merge_duplicates(self):\n",
    "        \"\"\"\n",
    "        Scans for normalization collisions and merges them into the \n",
    "        highest-degree node (The 'Alpha').\n",
    "        \"\"\"\n",
    "        print(\"\\nüî™ Starting Merge Operation...\")\n",
    "        \n",
    "        # 1. Map Keys to Nodes\n",
    "        normalization_map = defaultdict(list)\n",
    "        for node in self.G.nodes():\n",
    "            key = self._normalize_key(node)\n",
    "            if key:\n",
    "                normalization_map[key].append(node)\n",
    "        \n",
    "        merged_count = 0\n",
    "        nodes_removed = 0\n",
    "        \n",
    "        # 2. Iterate through clusters\n",
    "        for key, candidates in normalization_map.items():\n",
    "            if len(candidates) > 1:\n",
    "                # Sort by Degree (Highest first) -> The Alpha is index 0\n",
    "                candidates.sort(key=lambda x: self.G.degree(x), reverse=True)\n",
    "                \n",
    "                alpha_node = candidates[0] # The Winner (e.g., \"United States\")\n",
    "                beta_nodes = candidates[1:] # The Losers (e.g., \"United_States\")\n",
    "                \n",
    "                for beta in beta_nodes:\n",
    "                    self._transplant_edges(source=beta, target=alpha_node)\n",
    "                    self.G.remove_node(beta)\n",
    "                    nodes_removed += 1\n",
    "                \n",
    "                merged_count += 1\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"   ‚úÖ Operation Complete.\")\n",
    "        print(f\"   üîó Clusters Merged: {merged_count:,}\")\n",
    "        print(f\"   üóëÔ∏è Nodes Removed:   {nodes_removed:,}\")\n",
    "        print(f\"   üìâ Final Node Count: {self.G.number_of_nodes():,} (Was {self.initial_count:,})\")\n",
    "\n",
    "    def _transplant_edges(self, source, target):\n",
    "        \"\"\"\n",
    "        Moves all edges from Source to Target, then deletes Source.\n",
    "        \"\"\"\n",
    "        # Outgoing edges: source -> neighbor\n",
    "        for neighbor in list(self.G.successors(source)):\n",
    "            edge_data = self.G.get_edge_data(source, neighbor)\n",
    "            if not self.G.has_edge(target, neighbor):\n",
    "                self.G.add_edge(target, neighbor, **edge_data)\n",
    "        \n",
    "        # Incoming edges: neighbor -> source\n",
    "        for neighbor in list(self.G.predecessors(source)):\n",
    "            edge_data = self.G.get_edge_data(neighbor, source)\n",
    "            if not self.G.has_edge(neighbor, target):\n",
    "                self.G.add_edge(neighbor, target, **edge_data)\n",
    "\n",
    "    def save(self, output_path):\n",
    "        print(f\"\\nüíæ Saving Healthy Graph to {output_path}...\")\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pickle.dump(self.G, f)\n",
    "        print(\"   ‚úÖ Save Complete.\")\n",
    "\n",
    "    # def normalize_edge_relations(self):\n",
    "    #     \"\"\"\n",
    "    #     Standardizes edge names.\n",
    "    #     1. Explicit Mapping (e.g. 'is_located_in' -> 'located_in')\n",
    "    #     2. Format Standardization (snake_case, remove trailing 's')\n",
    "    #     3. De-duplication (if renamed edge already exists, delete the old one)\n",
    "    #     \"\"\"\n",
    "    #     print(\"\\nüõ†Ô∏è Starting Edge Normalization...\")\n",
    "        \n",
    "    #     # 1. DEFINE CANONICAL MAP (Safe Merges Only)\n",
    "    #     # Based on your Scan Results\n",
    "    #     mapping = {\n",
    "    #         \"is_located_in\": \"located_in\",\n",
    "    #         \"located in\": \"located_in\",\n",
    "    #         \"located_on\": \"located_in\", # Context dependent, but usually safe for RAG\n",
    "    #         \"includes\": \"include\",\n",
    "    #         \"included\": \"include\",\n",
    "    #         \"contains\": \"include\", # Semantic synonym\n",
    "    #         \"is_part_of\": \"part_of\",\n",
    "    #         \"part of\": \"part_of\",\n",
    "    #         \"released_in\": \"released\",\n",
    "    #         \"released_on\": \"released\",\n",
    "    #         \"is_released_in\": \"released\",\n",
    "    #         \"occurred_in\": \"occurred\",\n",
    "    #         \"occurred_on\": \"occurred\",\n",
    "    #         \"published_in\": \"published\",\n",
    "    #         \"published_on\": \"published\",\n",
    "    #         \"is_published_in\": \"published\",\n",
    "    #         \"was_born_in\": \"born_in\",\n",
    "    #         \"born in\": \"born_in\"\n",
    "    #     }\n",
    "        \n",
    "    #     normalized_count = 0\n",
    "    #     deleted_redundant = 0\n",
    "        \n",
    "    #     # Iterate over a static list of edges because we will modify the graph\n",
    "    #     # list(G.edges) gives (u, v) tuples\n",
    "    #     for u, v in list(self.G.edges()):\n",
    "    #         # Get the existing attributes\n",
    "    #         attrs = self.G.get_edge_data(u, v)\n",
    "    #         old_rel = attrs.get('relation', 'related_to')\n",
    "            \n",
    "    #         # Skip if not string\n",
    "    #         if not isinstance(old_rel, str): continue\n",
    "            \n",
    "    #         old_rel_lower = old_rel.lower().strip()\n",
    "    #         new_rel = old_rel_lower\n",
    "            \n",
    "    #         # --- APPLY RULES ---\n",
    "            \n",
    "    #         # Rule 1: Explicit Map\n",
    "    #         if old_rel_lower in mapping:\n",
    "    #             new_rel = mapping[old_rel_lower]\n",
    "            \n",
    "    #         # Rule 2: Syntactic Cleanup (snake_case)\n",
    "    #         new_rel = new_rel.replace(\" \", \"_\")\n",
    "            \n",
    "    #         # Rule 3: Naive Stemming (Remove trailing 's' if > 4 chars)\n",
    "    #         # e.g., \"releases\" -> \"release\", but keep \"is\", \"has\"\n",
    "    #         if new_rel.endswith(\"s\") and len(new_rel) > 4 and not new_rel.endswith(\"ss\"):\n",
    "    #             # specific check to avoid \"press\" -> \"pres\"\n",
    "    #             new_rel = new_rel[:-1]\n",
    "\n",
    "    #         # --- EXECUTE UPDATE ---\n",
    "            \n",
    "    #         if new_rel != old_rel:\n",
    "    #             # We need to change the relation name.\n",
    "    #             # BUT, does an edge with 'new_rel' already exist?\n",
    "                \n",
    "    #             # Check if we are creating a collision\n",
    "    #             # (Since G is a DiGraph, we can't have two edges u->v. \n",
    "    #             # But we can update the 'relation' attribute of the existing one)\n",
    "                \n",
    "    #             # Wait, DiGraph stores edge data in a dict.\n",
    "    #             # If we just update attributes: G[u][v]['relation'] = new_rel\n",
    "    #             # But what if G[u][v] ALREADY exists? \n",
    "    #             # In a DiGraph, there is only ONE edge between u and v. \n",
    "    #             # So we are just renaming that one edge's label.\n",
    "                \n",
    "    #             # However, if your graph WAS a MultiGraph (it isn't), this logic would be harder.\n",
    "    #             # Since it is a DiGraph, we just update the attribute.\n",
    "                \n",
    "    #             self.G[u][v]['relation'] = new_rel\n",
    "    #             normalized_count += 1\n",
    "                \n",
    "        # print(\"-\" * 50)\n",
    "        # print(f\"   ‚úÖ Normalization Complete.\")\n",
    "        # print(f\"   üè∑Ô∏è Relations Updated: {normalized_count:,}\")\n",
    "\n",
    "# Initialize\n",
    "surgeon = GraphSurgeon(GRAPH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dda589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî™ Starting Merge Operation...\n",
      "----------------------------------------\n",
      "   ‚úÖ Operation Complete.\n",
      "   üîó Clusters Merged: 15,396\n",
      "   üóëÔ∏è Nodes Removed:   17,103\n",
      "   üìâ Final Node Count: 294,133 (Was 311,236)\n",
      "\n",
      "üíæ Saving Healthy Graph to ./models/knowledge_graph_clean.pkl...\n",
      "   ‚úÖ Save Complete.\n",
      "\n",
      "‚ö†Ô∏è ACTION REQUIRED: Update your 'omni_rag_modules.py' to point to:\n",
      "   GRAPH_PATH = './models/knowledge_graph_clean.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 5: EXECUTE SURGERY & SAVE\n",
    "# ==========================================\n",
    "\n",
    "# 1. Perform the Merge\n",
    "surgeon.merge_duplicates()\n",
    "\n",
    "# 2. Save the Clean Version\n",
    "# We save to a NEW file to be safe.\n",
    "OUTPUT_PATH = \"./models/knowledge_graph_clean.pkl\"\n",
    "surgeon.save(OUTPUT_PATH)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è ACTION REQUIRED: Update your 'omni_rag_modules.py' to point to:\")\n",
    "print(f\"   GRAPH_PATH = '{OUTPUT_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6003ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Graph from ./models/knowledge_graph_clean.pkl...\n",
      "‚úÖ Graph Loaded. Nodes: 294,133 | Edges: 369,783\n",
      "------------------------------------------------------------\n",
      "üìä CONNECTIVITY STATISTICS\n",
      "   ‚Ä¢ Average Connections per Node: 2.51\n",
      "   ‚Ä¢ Median Connections:           1.0\n",
      "   ‚Ä¢ Max Degree (The King Node):   1904\n",
      "   ‚Ä¢ Isolated Nodes (Degree=0):    0\n",
      "------------------------------------------------------------\n",
      "üõë SUPER-NODE THRESHOLDS\n",
      "   ‚Ä¢ Top 5% Cutoff:   > 6.0 connections\n",
      "   ‚Ä¢ Top 1% Cutoff:   > 18.0 connections\n",
      "   ‚Ä¢ Top 0.1% Cutoff: > 143.0 connections\n",
      "\n",
      "   üëâ RECOMMENDATION: Set SUPER_NODE_THRESHOLD = 18\n",
      "------------------------------------------------------------\n",
      "üí£ LIST OF POTENTIAL 'BLACK HOLES' (Top 2920 Nodes)\n",
      "   (These are nodes we might need to blacklist or filter)\n",
      "                   Entity  Connections\n",
      "0           United States         1904\n",
      "1                  France          976\n",
      "2                  London          874\n",
      "3                   Egypt          832\n",
      "4                  Greece          824\n",
      "5                   Paris          803\n",
      "6                 Nanjing          776\n",
      "7                Portugal          735\n",
      "8                   China          731\n",
      "9                 Germany          718\n",
      "10  Arnold Schwarzenegger          675\n",
      "11            Switzerland          673\n",
      "12                Madonna          655\n",
      "13                Britain          653\n",
      "14          New York City          640\n",
      "15               Napoleon          631\n",
      "16                 Israel          620\n",
      "17                  Japan          615\n",
      "18           Philadelphia          607\n",
      "19                England          584\n",
      "------------------------------------------------------------\n",
      "üîó TOP 20 RELATIONSHIPS (Candidates for Stop-List)\n",
      "           Relation  Count\n",
      "0        located_in  14864\n",
      "1          includes   2358\n",
      "2           part_of   2348\n",
      "3      published_in   1908\n",
      "4       occurred_in   1882\n",
      "5                is   1845\n",
      "6          contains   1626\n",
      "7           used_in   1543\n",
      "8      published_by   1367\n",
      "9          released   1365\n",
      "10        published   1306\n",
      "11              won   1294\n",
      "12              has   1243\n",
      "13            wrote   1179\n",
      "14         Relation   1176\n",
      "15             is_a   1125\n",
      "16         used_for   1055\n",
      "17  associated_with   1030\n",
      "18              was   1025\n",
      "19      released_in   1014\n",
      "------------------------------------------------------------\n",
      "üè∑Ô∏è TOP ENTITY TYPES\n",
      "     Type   Count\n",
      "0  entity  294133\n",
      "------------------------------------------------------------\n",
      "‚úÖ SCAN COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GRAPH MRI SCANNER\n",
    "# ==========================================\n",
    "# 1. LOAD THE BRAIN\n",
    "print(f\"üìÇ Loading Graph from {OUTPUT_PATH}...\")\n",
    "\n",
    "with open(OUTPUT_PATH, \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Graph Loaded. Nodes: {G.number_of_nodes():,} | Edges: {G.number_of_edges():,}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 2. DEGREE DISTRIBUTION (The Mathematics of Connectivity)\n",
    "degrees = [d for n, d in G.degree()]\n",
    "in_degrees = [d for n, d in G.in_degree()]\n",
    "out_degrees = [d for n, d in G.out_degree()]\n",
    "\n",
    "print(\"üìä CONNECTIVITY STATISTICS\")\n",
    "print(f\"   ‚Ä¢ Average Connections per Node: {np.mean(degrees):.2f}\")\n",
    "print(f\"   ‚Ä¢ Median Connections:           {np.median(degrees):.1f}\")\n",
    "print(f\"   ‚Ä¢ Max Degree (The King Node):   {np.max(degrees)}\")\n",
    "print(f\"   ‚Ä¢ Isolated Nodes (Degree=0):    {degrees.count(0)}\")\n",
    "\n",
    "# 3. SUPER-NODE DETECTION (The 99% Cutoff)\n",
    "# We define a \"Super Node\" as anything in the top 1% or 0.5% of connectivity.\n",
    "p95 = np.percentile(degrees, 95)\n",
    "p99 = np.percentile(degrees, 99)\n",
    "p99_9 = np.percentile(degrees, 99.9)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"üõë SUPER-NODE THRESHOLDS\")\n",
    "print(f\"   ‚Ä¢ Top 5% Cutoff:   > {p95:.1f} connections\")\n",
    "print(f\"   ‚Ä¢ Top 1% Cutoff:   > {p99:.1f} connections\")\n",
    "print(f\"   ‚Ä¢ Top 0.1% Cutoff: > {p99_9:.1f} connections\")\n",
    "print(f\"\\n   üëâ RECOMMENDATION: Set SUPER_NODE_THRESHOLD = {int(p99)}\")\n",
    "\n",
    "# 4. IDENTIFYING THE \"BLACK HOLES\"\n",
    "# Let's see exactly WHO these super-nodes are.\n",
    "sorted_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "super_nodes = [n for n in sorted_nodes if n[1] > p99]\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"üí£ LIST OF POTENTIAL 'BLACK HOLES' (Top {len(super_nodes)} Nodes)\")\n",
    "print(\"   (These are nodes we might need to blacklist or filter)\")\n",
    "print(pd.DataFrame(super_nodes[:20], columns=[\"Entity\", \"Connections\"]))\n",
    "\n",
    "# 5. RELATIONSHIP AUDIT (Finding the \"Junk\")\n",
    "# We check which verbs are most common. If \"is\" appears 5000 times, we ban it.\n",
    "edge_attrs = nx.get_edge_attributes(G, \"relation\")\n",
    "relation_counts = Counter(edge_attrs.values())\n",
    "df_rel = pd.DataFrame(relation_counts.most_common(20), columns=[\"Relation\", \"Count\"])\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"üîó TOP 20 RELATIONSHIPS (Candidates for Stop-List)\")\n",
    "print(df_rel)\n",
    "\n",
    "# 6. ENTITY TYPE AUDIT (If available)\n",
    "# Checking what kind of things we have (Person, Place, etc.) if extracted\n",
    "node_types = nx.get_node_attributes(G, \"type\")\n",
    "if node_types:\n",
    "    type_counts = Counter(node_types.values())\n",
    "    print(\"-\" * 60)\n",
    "    print(\"üè∑Ô∏è TOP ENTITY TYPES\")\n",
    "    print(pd.DataFrame(type_counts.most_common(10), columns=[\"Type\", \"Count\"]))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No 'type' attribute found on nodes (Standard for pure extraction).\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"‚úÖ SCAN COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4e14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
