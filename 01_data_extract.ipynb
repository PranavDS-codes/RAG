{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab025ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pranavpant/Desktop/code /RAG/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38815fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wikipediaapi\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "\n",
    "# Define Paths\n",
    "DATA_DIR = \"./data/raw\"\n",
    "LOGS_DIR = \"./data/logs\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "WIKI_USER_AGENT = 'RAG_Portfolio_Builder/1.0 (your_email@example.com)'\n",
    "print(f\"‚úÖ Environment Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea9d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading SQuAD v2.0 (Train & Validation)...\n",
      "‚úÖ Merged Dataset Created.\n",
      "   - Train Size: 130319\n",
      "   - Valid Size: 11873\n",
      "   - Total Q&A Pairs: 142192\n",
      "üîÑ Organizing Metadata by Article Title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142192/142192 [00:01<00:00, 114998.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 477 unique articles across all splits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Both Splits\n",
    "print(\"üì• Loading SQuAD v2.0 (Train & Validation)...\")\n",
    "ds_train = load_dataset(\"squad_v2\", split=\"train\")\n",
    "ds_valid = load_dataset(\"squad_v2\", split=\"validation\")\n",
    "\n",
    "# 2. Convert to Pandas for easier tagging\n",
    "df_train = ds_train.to_pandas()\n",
    "df_valid = ds_valid.to_pandas()\n",
    "\n",
    "# 3. Add 'split' column\n",
    "df_train['split'] = 'train'\n",
    "df_valid['split'] = 'validation'\n",
    "\n",
    "# 4. Concatenate\n",
    "df_combined = pd.concat([df_train, df_valid], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Merged Dataset Created.\")\n",
    "print(f\"   - Train Size: {len(df_train)}\")\n",
    "print(f\"   - Valid Size: {len(df_valid)}\")\n",
    "print(f\"   - Total Q&A Pairs: {len(df_combined)}\")\n",
    "\n",
    "# 5. Extract Unique Titles (The Knowledge Base Target)\n",
    "# We aggregate ground truth contexts from both splits\n",
    "squad_metadata = {}\n",
    "\n",
    "print(\"üîÑ Organizing Metadata by Article Title...\")\n",
    "for index, row in tqdm(df_combined.iterrows(), total=len(df_combined), desc=\"Processing Rows\"):\n",
    "    title = row['title']\n",
    "    \n",
    "    if title not in squad_metadata:\n",
    "        squad_metadata[title] = {\n",
    "            \"squad_contexts\": set(),\n",
    "            \"qas_count\": 0\n",
    "        }\n",
    "    \n",
    "    squad_metadata[title][\"squad_contexts\"].add(row['context'])\n",
    "    squad_metadata[title][\"qas_count\"] += 1\n",
    "\n",
    "# Convert sets to lists\n",
    "for title in squad_metadata:\n",
    "    squad_metadata[title][\"squad_contexts\"] = list(squad_metadata[title][\"squad_contexts\"])\n",
    "\n",
    "unique_titles = list(squad_metadata.keys())\n",
    "print(f\"üìö Found {len(unique_titles)} unique articles across all splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce965977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Extraction for 477 articles...\n",
      "‚òïÔ∏è Estimated time: 15-20 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  37%|‚ñà‚ñà‚ñà‚ñã      | 175/477 [05:47<10:55,  2.17s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bill_%26_Melinda_Gates_Foundation, Decoded: Bill_&_Melinda_Gates_Foundation and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 241/477 [08:00<07:42,  1.96s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Molotov%E2%80%93Ribbentrop_Pact, Decoded: Molotov‚ÄìRibbentrop_Pact and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 289/477 [09:36<06:20,  2.02s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: St._John%27s,_Newfoundland_and_Labrador, Decoded: St._John's,_Newfoundland_and_Labrador and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 347/477 [11:34<04:34,  2.11s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Seven_Years%27_War, Decoded: Seven_Years'_War and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 396/477 [13:12<02:38,  1.96s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Kievan_Rus%27, Decoded: Kievan_Rus' and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 417/477 [13:55<02:03,  2.05s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bras%C3%ADlia, Decoded: Bras√≠lia and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 421/477 [14:02<01:49,  1.95s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Jehovah%27s_Witnesses, Decoded: Jehovah's_Witnesses and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 424/477 [14:09<01:47,  2.03s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Financial_crisis_of_2007%E2%80%9308, Decoded: Financial_crisis_of_2007‚Äì08 and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 430/477 [14:20<01:31,  1.95s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Saint_Barth%C3%A9lemy, Decoded: Saint_Barth√©lemy and Status: SUCCESS_FALLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 477/477 [15:54<00:00,  2.00s/article]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "‚ú® Knowledge Base Saved: ./data/raw/knowledge_base_raw.json\n",
      "üìã Detailed Logs Saved:  ./data/logs/extraction_log_combined_20251215_185559.csv\n",
      "========================================\n",
      "status\n",
      "SUCCESS             468\n",
      "SUCCESS_FALLBACK      9\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "                                            title   status  word_count  \\\n",
      "0                                         Beyonc√©  SUCCESS        9446   \n",
      "1                                 Fr√©d√©ric_Chopin  SUCCESS       10662   \n",
      "2  Sino-Tibetan_relations_during_the_Ming_dynasty  SUCCESS       10961   \n",
      "3                                            IPod  SUCCESS        5999   \n",
      "4          The_Legend_of_Zelda:_Twilight_Princess  SUCCESS        5065   \n",
      "\n",
      "   fetch_duration_sec  \n",
      "0                0.54  \n",
      "1                0.51  \n",
      "2                0.38  \n",
      "3                0.58  \n",
      "4                0.38  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Wikipedia API\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent=WIKI_USER_AGENT,\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "full_knowledge_base = []\n",
    "process_logs = []\n",
    "\n",
    "print(f\"üöÄ Starting Extraction for {len(unique_titles)} articles...\")\n",
    "print(\"‚òïÔ∏è Estimated time: 15-20 minutes.\")\n",
    "\n",
    "for i, title in enumerate(tqdm(unique_titles, desc=\"Fetching Articles\", unit=\"article\")):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize Defaults\n",
    "    status = \"FAILED\"\n",
    "    url = \"N/A\"\n",
    "    word_count = 0\n",
    "    char_count = 0\n",
    "    est_chunks = 0\n",
    "    squad_qas = 0\n",
    "    error_details = \"None\"\n",
    "    \n",
    "    # We will use this to track if we had to use the fallback method\n",
    "    final_display_title = title \n",
    "    \n",
    "    try:\n",
    "        # 1. Retrieve Metadata\n",
    "        if title in squad_metadata:\n",
    "            squad_qas = squad_metadata[title][\"qas_count\"]\n",
    "        \n",
    "        # 2. Attempt 1: Fetch with original SQuAD title\n",
    "        page = wiki.page(title)\n",
    "        \n",
    "        # 3. Attempt 2: Fallback (URL Decode) if Attempt 1 failed\n",
    "        if not page.exists():\n",
    "            decoded_title = urllib.parse.unquote(title)\n",
    "            \n",
    "            # Only try again if decoding actually changed the string\n",
    "            if decoded_title != title:\n",
    "                # Try fetching with the clean title\n",
    "                fallback_page = wiki.page(decoded_title)\n",
    "                if fallback_page.exists():\n",
    "                    page = fallback_page\n",
    "                    final_display_title = decoded_title # Update for display\n",
    "                    status = \"SUCCESS_FALLBACK\" # Mark that we fixed it\n",
    "                    print(f\"Title: {title}, Decoded: {decoded_title} and Status: {status}\")\n",
    "        \n",
    "        # 4. Process Page (Whether from Attempt 1 or Attempt 2)\n",
    "        if page.exists():\n",
    "            if status == \"FAILED\": status = \"SUCCESS\"\n",
    "            \n",
    "            url = page.fullurl\n",
    "            text_content = page.text\n",
    "            \n",
    "            # Calculate Metrics\n",
    "            word_count = len(text_content.split())\n",
    "            char_count = len(text_content)\n",
    "            est_chunks = max(1, word_count // 500)\n",
    "            \n",
    "            doc_entry = {\n",
    "                \"id\": title,               # CRITICAL: Keep original encoded ID to match SQuAD Questions!\n",
    "                \"title\": final_display_title, # Human-readable title\n",
    "                \"source_url\": url,\n",
    "                \"full_text\": text_content,\n",
    "                \"summary\": page.summary,\n",
    "                \"squad_ground_truth\": {\n",
    "                    \"original_contexts\": squad_metadata.get(title, {}).get(\"squad_contexts\", []),\n",
    "                    \"qas_count\": squad_qas\n",
    "                },\n",
    "                \"fetched_at\": datetime.now().isoformat()\n",
    "            }\n",
    "            full_knowledge_base.append(doc_entry)\n",
    "        else:\n",
    "            status = \"NOT_FOUND\"\n",
    "            error_details = \"Page.exists() returned False (tried raw & decoded)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"ERROR\"\n",
    "        error_details = str(e)\n",
    "        print(f\"Error fetching {title}: {e}\")\n",
    "    \n",
    "    # Calculate Duration\n",
    "    end_time = time.time()\n",
    "    duration = round(end_time - start_time, 2)\n",
    "\n",
    "    # Append Detailed Log\n",
    "    process_logs.append({\n",
    "        \"title\": title, # Log the requested title\n",
    "        \"status\": status,\n",
    "        \"url\": url,\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"estimated_chunks\": est_chunks,\n",
    "        \"squad_qas_available\": squad_qas,\n",
    "        \"fetch_duration_sec\": duration,\n",
    "        \"error_details\": error_details\n",
    "    })\n",
    "\n",
    "    # Polite Sleep\n",
    "    time.sleep(1.5) \n",
    "\n",
    "    # Checkpoint every 50\n",
    "    if (i + 1) % 50 == 0:\n",
    "        with open(f\"{DATA_DIR}/knowledge_base_CHECKPOINT.json\", \"w\", encoding='utf-8') as f:\n",
    "            json.dump(full_knowledge_base, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Final Save JSON\n",
    "kb_path = f\"{DATA_DIR}/knowledge_base_raw.json\"\n",
    "with open(kb_path, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(full_knowledge_base, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Final Save Logs (CSV)\n",
    "df_logs = pd.DataFrame(process_logs)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = f\"{LOGS_DIR}/extraction_log_combined_{timestamp}.csv\"\n",
    "df_logs.to_csv(log_path, index=False)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"‚ú® Knowledge Base Saved: {kb_path}\")\n",
    "print(f\"üìã Detailed Logs Saved:  {log_path}\")\n",
    "print(\"=\"*40)\n",
    "# Show success vs fallback stats\n",
    "print(df_logs['status'].value_counts())\n",
    "print(\"-\" * 40)\n",
    "print(df_logs[['title', 'status', 'word_count', 'fetch_duration_sec']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea934de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Building Master Evaluation Set...\n",
      "============================================================\n",
      "üéâ MASTER DATASET GENERATION COMPLETE\n",
      "============================================================\n",
      "üìö KNOWLEDGE BASE METRICS\n",
      "   - File Path:      ./data/raw/knowledge_base_raw.json\n",
      "   - Size on Disk:   41.24 MB\n",
      "   - Total Articles: 477\n",
      "   - Total Words:    4,087,337 words\n",
      "   - Unique Titles:  477\n",
      "----------------------------------------\n",
      "üìù MASTER EXAM (EVALUATION SET) METRICS\n",
      "   - File Path:      ./data/raw/squad_eval_set_all.json\n",
      "   - Size on Disk:   157.80 MB\n",
      "   - Total Q&A Pairs:142192\n",
      "   - Covered Titles: 477 (Matched against KB)\n",
      "     ‚Ä¢ Train Split:  130319\n",
      "     ‚Ä¢ Val Split:    11873\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Helper: Fix for \"ndarray is not JSON serializable\" ---\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, (np.bool_, np.integer)):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(\"üîç Building Master Evaluation Set...\")\n",
    "\n",
    "# 1. Analyze Knowledge Base\n",
    "# We load the KB to filter our Q&A, but also to calculate stats\n",
    "if os.path.exists(kb_path):\n",
    "    with open(kb_path, \"r\", encoding='utf-8') as f:\n",
    "        kb_data = json.load(f)\n",
    "    \n",
    "    # Calculate KB Metrics\n",
    "    kb_titles = {doc['id'] for doc in kb_data}\n",
    "    kb_total_articles = len(kb_data)\n",
    "    # Count total words across all articles in the KB\n",
    "    kb_total_words = sum(len(doc.get('full_text', \"\").split()) for doc in kb_data)\n",
    "    # Get file size in MB\n",
    "    kb_size_mb = os.path.getsize(kb_path) / (1024 * 1024)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: Knowledge base not found at {kb_path}. Master set will be empty.\")\n",
    "    kb_data = []\n",
    "    kb_titles = set()\n",
    "    kb_total_articles = 0\n",
    "    kb_total_words = 0\n",
    "    kb_size_mb = 0\n",
    "\n",
    "# 2. Filter the Combined DataFrame\n",
    "# Keep only questions where the corresponding article exists in our downloaded KB\n",
    "df_final_eval = df_combined[df_combined['title'].isin(kb_titles)].copy()\n",
    "\n",
    "# 3. Add 'is_impossible' helper\n",
    "df_final_eval['is_impossible'] = df_final_eval['answers'].apply(lambda x: len(x['text']) == 0)\n",
    "\n",
    "# 4. Convert ALL columns to dictionary\n",
    "eval_data = df_final_eval.to_dict(orient='records')\n",
    "\n",
    "# 5. Save\n",
    "eval_path = f\"{DATA_DIR}/squad_eval_set_all.json\"\n",
    "with open(eval_path, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(eval_data, f, ensure_ascii=False, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "# 6. Calculate Final Exam Metrics\n",
    "exam_total_q = len(eval_data)\n",
    "exam_train_q = len([x for x in eval_data if x.get('split')=='train'])\n",
    "exam_val_q = len([x for x in eval_data if x.get('split')=='validation'])\n",
    "exam_unique_titles = len(set(x['title'] for x in eval_data))\n",
    "exam_size_mb = os.path.getsize(eval_path) / (1024 * 1024) if os.path.exists(eval_path) else 0\n",
    "\n",
    "# 7. Print Detailed Report\n",
    "print(\"=\"*60)\n",
    "print(f\"üéâ MASTER DATASET GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üìö KNOWLEDGE BASE METRICS\")\n",
    "print(f\"   - File Path:      {kb_path}\")\n",
    "print(f\"   - Size on Disk:   {kb_size_mb:.2f} MB\")\n",
    "print(f\"   - Total Articles: {kb_total_articles}\")\n",
    "print(f\"   - Total Words:    {kb_total_words:,.0f} words\")\n",
    "print(f\"   - Unique Titles:  {len(kb_titles)}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"üìù MASTER EXAM (EVALUATION SET) METRICS\")\n",
    "print(f\"   - File Path:      {eval_path}\")\n",
    "print(f\"   - Size on Disk:   {exam_size_mb:.2f} MB\")\n",
    "print(f\"   - Total Q&A Pairs:{exam_total_q}\")\n",
    "print(f\"   - Covered Titles: {exam_unique_titles} (Matched against KB)\")\n",
    "print(f\"     ‚Ä¢ Train Split:  {exam_train_q}\")\n",
    "print(f\"     ‚Ä¢ Val Split:    {exam_val_q}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4663e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ INSPECTING: Wikipedia Knowledge Base\n",
      "   Path: ./data/raw/knowledge_base_raw.json\n",
      "   üìä Found 477 records.\n",
      "   ‚úÖ SUCCESS: Wikipedia Knowledge Base looks healthy and ready for RAG.\n",
      "--------------------------------------------------\n",
      "üî¨ INSPECTING: SQuAD Master Eval Set\n",
      "   Path: ./data/raw/squad_eval_set_all.json\n",
      "   üìä Found 142192 records.\n",
      "   ‚úÖ SUCCESS: SQuAD Master Eval Set looks healthy and ready for RAG.\n",
      "--------------------------------------------------\n",
      "\n",
      "üöÄ ALL SYSTEMS GO! Datasets are verified.\n"
     ]
    }
   ],
   "source": [
    "def validate_dataset(file_path, dataset_name, expected_columns, min_records=1):\n",
    "    print(f\"üî¨ INSPECTING: {dataset_name}\")\n",
    "    print(f\"   Path: {file_path}\")\n",
    "    \n",
    "    # 1. Check File Existence\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"   ‚ùå FATAL: File not found at {file_path}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Check JSON Validity\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"   ‚ùå FATAL: Invalid JSON format. {str(e)}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå FATAL: Error reading file. {str(e)}\")\n",
    "        return False\n",
    "        \n",
    "    # 3. Check Data Type (Should be a List of Dictionaries)\n",
    "    if not isinstance(data, list):\n",
    "        print(f\"   ‚ùå FAIL: Expected a list of records, got {type(data)}\")\n",
    "        return False\n",
    "        \n",
    "    # 4. Check Volume\n",
    "    count = len(data)\n",
    "    print(f\"   üìä Found {count} records.\")\n",
    "    if count < min_records:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Dataset is suspiciously small (< {min_records} records).\")\n",
    "    \n",
    "    # 5. Schema & Content Validation\n",
    "    missing_keys = set()\n",
    "    null_values = set()\n",
    "    empty_strings = set()\n",
    "    \n",
    "    # Check a sample (or all if small) to avoid freezing on massive datasets\n",
    "    sample_size = min(len(data), 5000) \n",
    "    sample_indices = random.sample(range(len(data)), sample_size)\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        item = data[i]\n",
    "        \n",
    "        # Check for missing keys\n",
    "        for col in expected_columns:\n",
    "            if col not in item:\n",
    "                missing_keys.add(col)\n",
    "                continue\n",
    "            \n",
    "            # Check for nulls\n",
    "            val = item[col]\n",
    "            if val is None:\n",
    "                null_values.add(col)\n",
    "            \n",
    "            # Check for empty strings (critical for text/context)\n",
    "            if isinstance(val, str) and len(val.strip()) == 0:\n",
    "                empty_strings.add(col)\n",
    "            \n",
    "            # Special Check: Answers in SQuAD\n",
    "            if col == 'answers' and isinstance(val, dict):\n",
    "                if 'text' not in val or not isinstance(val['text'], list):\n",
    "                     print(f\"   ‚ùå FAIL: Record {i} has invalid 'answers' format.\")\n",
    "                     return False\n",
    "\n",
    "    # 6. Final Report\n",
    "    is_valid = True\n",
    "    \n",
    "    if missing_keys:\n",
    "        print(f\"   ‚ùå FAIL: Missing columns in some records: {missing_keys}\")\n",
    "        is_valid = False\n",
    "    \n",
    "    if null_values:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Found NULL values in columns: {null_values}\")\n",
    "    \n",
    "    if empty_strings:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Found EMPTY STRINGS in columns: {empty_strings}\")\n",
    "        # Context being empty is usually critical\n",
    "        if 'context' in empty_strings or 'full_text' in empty_strings:\n",
    "            print(f\"   ‚ùå CRITICAL: Some records have empty context/text!\")\n",
    "            is_valid = False\n",
    "\n",
    "    if is_valid:\n",
    "        print(f\"   ‚úÖ SUCCESS: {dataset_name} looks healthy and ready for RAG.\")\n",
    "    else:\n",
    "        print(f\"   üõë FAILED: {dataset_name} needs fixing.\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    return is_valid\n",
    "\n",
    "# ==========================================\n",
    "# RUN CHECKS\n",
    "# ==========================================\n",
    "\n",
    "# 1. Check Wikipedia Knowledge Base\n",
    "# UPDATED: Changed 'text' -> 'full_text' and 'url' -> 'source_url'\n",
    "kb_valid = validate_dataset(\n",
    "    file_path=kb_path, \n",
    "    dataset_name=\"Wikipedia Knowledge Base\",\n",
    "    expected_columns=['id', 'title', 'full_text', 'source_url'] \n",
    ")\n",
    "\n",
    "# 2. Check SQuAD Evaluation Set\n",
    "# UPDATED: Added 'is_impossible' to check list\n",
    "squad_valid = validate_dataset(\n",
    "    file_path=eval_path,\n",
    "    dataset_name=\"SQuAD Master Eval Set\",\n",
    "    expected_columns=['id', 'title', 'context', 'question', 'answers', 'split', 'is_impossible']\n",
    ")\n",
    "\n",
    "if kb_valid and squad_valid:\n",
    "    print(\"\\nüöÄ ALL SYSTEMS GO! Datasets are verified.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è ISSUES DETECTED. Please review the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c43741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting Knowledge Base Augmentation...\n",
      "   ‚Ä¢ Mapping SQuAD contexts...\n",
      "   ‚Ä¢ Augmenting 477 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 477/477 [00:00<00:00, 2291.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Saving Augmented KB to ./data/raw/knowledge_base_augmented.json...\n",
      "==================================================\n",
      "üöÄ AUGMENTATION COMPLETE\n",
      "==================================================\n",
      "üìö Articles Processed: 477\n",
      "üîç Contexts Checked:   20233\n",
      "‚ö†Ô∏è Contexts Missing:   18337 (These would have caused RAG failure!)\n",
      "‚úÖ Contexts Injected:  18337\n",
      "==================================================\n",
      "The file './data/raw/knowledge_base_augmented.json' now contains the definitive\n",
      "source text tailored for your SQuAD evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Files\n",
    "kb_path = \"./data/raw/knowledge_base_raw.json\"\n",
    "eval_path = \"./data/raw/squad_eval_set_all.json\"\n",
    "output_path = \"./data/raw/knowledge_base_augmented.json\"\n",
    "\n",
    "print(\"üîÑ Starting Knowledge Base Augmentation...\")\n",
    "\n",
    "# 1. Load Datasets\n",
    "with open(kb_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    kb_data = json.load(f)\n",
    "\n",
    "with open(eval_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# 2. Map SQuAD Contexts by Title\n",
    "# We want to know: \"For the article 'Beyonc√©', what are all the paragraphs SQuAD expects?\"\n",
    "print(\"   ‚Ä¢ Mapping SQuAD contexts...\")\n",
    "squad_contexts_map = {}\n",
    "\n",
    "for row in squad_data:\n",
    "    title = row['title']\n",
    "    context = row['context']\n",
    "    \n",
    "    if title not in squad_contexts_map:\n",
    "        squad_contexts_map[title] = set()\n",
    "    \n",
    "    squad_contexts_map[title].add(context)\n",
    "\n",
    "# 3. Augment Knowledge Base\n",
    "# We iterate through our downloaded articles and inject missing contexts\n",
    "stats = {\n",
    "    \"articles_processed\": 0,\n",
    "    \"contexts_checked\": 0,\n",
    "    \"contexts_missing\": 0,\n",
    "    \"contexts_added\": 0\n",
    "}\n",
    "\n",
    "augmented_kb = []\n",
    "\n",
    "print(f\"   ‚Ä¢ Augmenting {len(kb_data)} articles...\")\n",
    "\n",
    "for article in tqdm(kb_data):\n",
    "    title = article['id'] # We use 'id' because that's what we matched SQuAD with earlier\n",
    "    current_text = article['full_text']\n",
    "    \n",
    "    # Check if we have SQuAD contexts for this title\n",
    "    if title in squad_contexts_map:\n",
    "        # Get all unique paragraphs SQuAD expects for this title\n",
    "        required_contexts = squad_contexts_map[title]\n",
    "        \n",
    "        missing_contexts = []\n",
    "        \n",
    "        for ctx in required_contexts:\n",
    "            stats['contexts_checked'] += 1\n",
    "            \n",
    "            # NORMALIZATION CHECK\n",
    "            # We strip whitespace to avoid false negatives due to formatting\n",
    "            if ctx.strip() not in current_text:\n",
    "                missing_contexts.append(ctx)\n",
    "                stats['contexts_missing'] += 1\n",
    "        \n",
    "        # INJECTION\n",
    "        if missing_contexts:\n",
    "            # We append missing contexts to the end of the article\n",
    "            # We add a clear separator so we know this is historical data\n",
    "            injection_text = \"\\n\\n\" + \"\\n\\n\".join(missing_contexts)\n",
    "            article['full_text'] += injection_text\n",
    "            stats['contexts_added'] += len(missing_contexts)\n",
    "            \n",
    "            # Optional: Add a metadata flag saying we modified this\n",
    "            article['augmented'] = True\n",
    "            article['augmented_count'] = len(missing_contexts)\n",
    "    \n",
    "    augmented_kb.append(article)\n",
    "    stats['articles_processed'] += 1\n",
    "\n",
    "# 4. Save\n",
    "print(f\"   ‚Ä¢ Saving Augmented KB to {output_path}...\")\n",
    "with open(output_path, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(augmented_kb, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 5. Report\n",
    "print(\"=\"*50)\n",
    "print(\"üöÄ AUGMENTATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìö Articles Processed: {stats['articles_processed']}\")\n",
    "print(f\"üîç Contexts Checked:   {stats['contexts_checked']}\")\n",
    "print(f\"‚ö†Ô∏è Contexts Missing:   {stats['contexts_missing']} (These would have caused RAG failure!)\")\n",
    "print(f\"‚úÖ Contexts Injected:  {stats['contexts_added']}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"The file '{output_path}' now contains the definitive\")\n",
    "print(\"source text tailored for your SQuAD evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
