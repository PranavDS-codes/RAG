{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f912e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Ready to build the Retrieval Engine.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 1: SETUP & IMPORTS\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "KB_PATH = \"./data/raw/knowledge_base_augmented.json\"\n",
    "MODELS_DIR = \"./models\"\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "MODEL_TOKEN_LIMIT = 512\n",
    "EST_CHAR_PER_TOKEN = 4\n",
    "SAFE_CHAR_LIMIT = MODEL_TOKEN_LIMIT * EST_CHAR_PER_TOKEN\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete. Ready to build the Retrieval Engine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ab571c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing Paragraph Distributions...\n",
      "\n",
      "üìä DATASET STATISTICS (Paragraphs)\n",
      "   ‚Ä¢ Total Paragraphs:     38566\n",
      "   ‚Ä¢ Average Length:       1038 chars\n",
      "   ‚Ä¢ Median Length:        774 chars\n",
      "   ‚Ä¢ Largest Paragraph:    12798 chars\n",
      "   ‚Ä¢ 99th Percentile:      4322 chars\n",
      "----------------------------------------\n",
      "ü§ñ MODEL CONSTRAINTS (mxbai-embed-large-v1)\n",
      "   ‚Ä¢ Max Context:          512 tokens\n",
      "   ‚Ä¢ Est. Char Limit:      ~2048 chars\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è WARNING: Found 4010 paragraphs larger than the model limit.\n",
      "   The largest is 12798 chars (approx 3200 tokens).\n",
      "   Example: 'Beyonc√© Giselle Knowles-Carter (  bee-ON-say; born September 4, 1981) is an American singer, songwri...' from Beyonc√©\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Analyzing Paragraph Distributions...\")\n",
    "\n",
    "# 1. Load Data\n",
    "with open(KB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    kb_data = json.load(f)\n",
    "\n",
    "# 2. Extract Paragraphs\n",
    "all_paragraph_lengths = []\n",
    "oversized_paragraphs = []\n",
    "\n",
    "for doc in kb_data:\n",
    "    # We assume double newline is the standard paragraph separator\n",
    "    paragraphs = doc['full_text'].split('\\n\\n')\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        # Strip whitespace\n",
    "        clean_p = p.strip()\n",
    "        p_len = len(clean_p)\n",
    "        \n",
    "        # Filter out empty or tiny artifacts (like headers or stray newlines)\n",
    "        if p_len > 50: \n",
    "            all_paragraph_lengths.append(p_len)\n",
    "            \n",
    "            # Record if it exceeds our model's safety limit\n",
    "            if p_len > SAFE_CHAR_LIMIT:\n",
    "                oversized_paragraphs.append({\n",
    "                    \"doc\": doc['title'],\n",
    "                    \"length\": p_len,\n",
    "                    \"preview\": clean_p[:100] + \"...\"\n",
    "                })\n",
    "\n",
    "# 3. Calculate Stats\n",
    "max_p = np.max(all_paragraph_lengths)\n",
    "avg_p = np.mean(all_paragraph_lengths)\n",
    "median_p = np.median(all_paragraph_lengths)\n",
    "percentile_99 = np.percentile(all_paragraph_lengths, 99)\n",
    "\n",
    "# 4. Report\n",
    "print(f\"\\nüìä DATASET STATISTICS (Paragraphs)\")\n",
    "print(f\"   ‚Ä¢ Total Paragraphs:     {len(all_paragraph_lengths)}\")\n",
    "print(f\"   ‚Ä¢ Average Length:       {avg_p:.0f} chars\")\n",
    "print(f\"   ‚Ä¢ Median Length:        {median_p:.0f} chars\")\n",
    "print(f\"   ‚Ä¢ Largest Paragraph:    {max_p} chars\")\n",
    "print(f\"   ‚Ä¢ 99th Percentile:      {percentile_99:.0f} chars\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"ü§ñ MODEL CONSTRAINTS (mxbai-embed-large-v1)\")\n",
    "print(f\"   ‚Ä¢ Max Context:          {MODEL_TOKEN_LIMIT} tokens\")\n",
    "print(f\"   ‚Ä¢ Est. Char Limit:      ~{SAFE_CHAR_LIMIT} chars\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 5. The Verdict\n",
    "if max_p > SAFE_CHAR_LIMIT:\n",
    "    print(f\"‚ö†Ô∏è WARNING: Found {len(oversized_paragraphs)} paragraphs larger than the model limit.\")\n",
    "    print(f\"   The largest is {max_p} chars (approx {max_p/4:.0f} tokens).\")\n",
    "    print(f\"   Example: '{oversized_paragraphs[0]['preview']}' from {oversized_paragraphs[0]['doc']}\")\n",
    "else:\n",
    "    print(f\"‚úÖ SUCCESS: All paragraphs fit within the model window!\")\n",
    "    print(\"   Simple paragraph splitting is safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965bdca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî™ Starting Chunking Process...\n",
      "üìö Processing 477 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 477/477 [00:00<00:00, 3999.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "‚úÖ CHUNKING COMPLETE\n",
      "   ‚Ä¢ Total Chunks: 39141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 2: INTELLIGENT CHUNKING\n",
    "# ==========================================\n",
    "print(\"üî™ Starting Chunking Process...\")\n",
    "\n",
    "# Strategy: 1000 chars is approx 250 tokens. \n",
    "# This fits easily into bge-small's 512 token limit.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=250,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], \n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Load Data\n",
    "with open(KB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    kb_data = json.load(f)\n",
    "\n",
    "chunks = []\n",
    "doc_id_counter = 0\n",
    "\n",
    "print(f\"üìö Processing {len(kb_data)} articles...\")\n",
    "\n",
    "for doc in tqdm(kb_data, desc=\"Chunking\"):\n",
    "    title = doc['title']\n",
    "    full_text = doc['full_text']\n",
    "    url = doc['source_url']\n",
    "    original_id = doc['id']\n",
    "    \n",
    "    doc_splits = text_splitter.split_text(full_text)\n",
    "    \n",
    "    for i, split_text in enumerate(doc_splits):\n",
    "        chunks.append({\n",
    "            \"chunk_id\": doc_id_counter,\n",
    "            \"doc_id\": original_id,\n",
    "            \"title\": title,\n",
    "            \"text\": split_text,\n",
    "            \"source_url\": url,\n",
    "            \"chunk_index\": i\n",
    "        })\n",
    "        doc_id_counter += 1\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚úÖ CHUNKING COMPLETE\")\n",
    "print(f\"   ‚Ä¢ Total Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c0c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading Model: BAAI/bge-small-en-v1.5...\n",
      "‚ö° Encoding 39141 chunks (Dimensions: 384)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04b1d112a8e4a45a5aa99b9463fe126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "‚úÖ EMBEDDING COMPLETE\n",
      "   ‚Ä¢ Matrix Shape: (39141, 384)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 3: VECTORIZATION\n",
    "# ==========================================\n",
    "print(f\"üß† Loading Model: {EMBEDDING_MODEL_NAME}...\")\n",
    "\n",
    "# 1. Load Model\n",
    "# bge-small is highly efficient and runs great on local CPUs/MPS\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# 2. Batch Encoding\n",
    "batch_size = 64  # Increased batch size since model is smaller\n",
    "all_texts = [c['text'] for c in chunks]\n",
    "\n",
    "print(f\"‚ö° Encoding {len(chunks)} chunks (Dimensions: 384)...\")\n",
    "\n",
    "embeddings = model.encode(\n",
    "    all_texts,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True # CRITICAL: bge models need normalization for dot product\n",
    ")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚úÖ EMBEDDING COMPLETE\")\n",
    "print(f\"   ‚Ä¢ Matrix Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "153b4166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è Building FAISS Index...\n",
      "   ‚Ä¢ Index contains 39141 vectors.\n",
      "----------------------------------------\n",
      "üéâ SYSTEM PERSISTED SUCCESSFULLY\n",
      "   ‚Ä¢ Index File:    ./models/faiss_index.bin\n",
      "   ‚Ä¢ Metadata File: ./models/chunk_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 4: INDEXING & PERSISTENCE\n",
    "# ==========================================\n",
    "print(\"üóÑÔ∏è Building FAISS Index...\")\n",
    "\n",
    "# 1. Initialize FAISS Index\n",
    "# BGE-Small produces 384-dimensional vectors\n",
    "dimension = 384 \n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# 2. Add Vectors\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"   ‚Ä¢ Index contains {index.ntotal} vectors.\")\n",
    "\n",
    "# 3. Save Artifacts\n",
    "index_path = f\"{MODELS_DIR}/faiss_index.bin\"\n",
    "metadata_path = f\"{MODELS_DIR}/chunk_metadata.pkl\"\n",
    "\n",
    "faiss.write_index(index, index_path)\n",
    "\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"üéâ SYSTEM PERSISTED SUCCESSFULLY\")\n",
    "print(f\"   ‚Ä¢ Index File:    {index_path}\")\n",
    "print(f\"   ‚Ä¢ Metadata File: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c360fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü©∫ RUNNING HEALTH CHECK...\n",
      "\n",
      "‚ùì Test Query: 'Who managed Destiny's Child?'\n",
      "----------------------------------------\n",
      "ü•á Rank 1 (Score: 0.6261)\n",
      "   Source: Beyonc√©\n",
      "   Text:   Following several lineup changes, Destiny's Child ultimately comprised Beyonc√©, Rowland, and Michelle Williams. In early 2001, while the group were co...\n",
      "\n",
      "ü•á Rank 2 (Score: 0.6242)\n",
      "   Source: Beyonc√©\n",
      "   Text:   In November 2003, Beyonc√© embarked on the European Dangerously in Love Tour and North American Verizon Ladies First Tour alongside Missy Elliott and A...\n",
      "\n",
      "ü•á Rank 3 (Score: 0.6144)\n",
      "   Source: Beyonc√©\n",
      "   Text:   Beyonc√© Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (n√©e Beyinc√©), a hairdresser and salon owner, and Mathew Knowles, ...\n",
      "\n",
      "ü•á Rank 4 (Score: 0.6084)\n",
      "   Source: Beyonc√©\n",
      "   Text:   On January 7, 2012, Beyonc√© gave birth to a daughter, Blue Ivy Carter, at Lenox Hill Hospital in New York under heavy security. Two days later, Jay Z ...\n",
      "\n",
      "ü•á Rank 5 (Score: 0.6001)\n",
      "   Source: Beyonc√©\n",
      "   Text:   In October 2014, it was announced that Beyonc√© with her management company Parkwood Entertainment would be partnering with London-based fashion retail...\n",
      "\n",
      "‚úÖ Retrieval Engine is ONLINE.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 5: HEALTH CHECK\n",
    "# ==========================================\n",
    "print(\"ü©∫ RUNNING HEALTH CHECK...\")\n",
    "\n",
    "query_text = \"Who managed Destiny's Child?\"\n",
    "\n",
    "# BGE models use this specific instruction for queries for best results\n",
    "query_prompt = f\"Represent this sentence for searching relevant passages: {query_text}\"\n",
    "query_vector = model.encode([query_prompt], normalize_embeddings=True)\n",
    "\n",
    "k = 5\n",
    "D, I = index.search(query_vector, k)\n",
    "\n",
    "print(f\"\\n‚ùì Test Query: '{query_text}'\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(k):\n",
    "    idx = I[0][i]\n",
    "    score = D[0][i]\n",
    "    retrieved_chunk = chunks[idx]\n",
    "    \n",
    "    print(f\"ü•á Rank {i+1} (Score: {score:.4f})\")\n",
    "    print(f\"   Source: {retrieved_chunk['title']}\")\n",
    "    print(f\"   Text:   {retrieved_chunk['text'][:150]}...\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"‚úÖ Retrieval Engine is ONLINE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe198c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
